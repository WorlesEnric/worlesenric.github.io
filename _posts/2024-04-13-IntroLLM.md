---
layout: post
usehighlight: true
tags: [LLM, introduction, NLP]
title: Large Language Models - Aspects and Methods
---


This post is the note for reviewing the development of the Large Language Models (LLMs), for more detailed and rigorous text, please refer to the monographs.

**Warning:** This post should not serve as an introduction text to whom may not be familiar with the deep learning methodologies, since it contains intensive personal opinions, not assured to be correct.

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Sequence-to-sequence Learning via RNN](#sequence-to-sequence-learning-via-rnn)
    - [Recurrent Neural Network and Long Short Term Memory Network](#recurrent-neural-network-and-long-short-term-memory-network)
    - [Encoder-Decoder Architecture](#encoder-decoder-architecture)
    - [Self-Attention and Transformer](#self-attention-and-transformer)

## Sequence-to-sequence Learning via RNN

Despite the purpose of giving an introduction of neural language processing, we will start from reviewing  the machine translation (MT) task, for the simplicity  of the ideas -- transfrom one sequence of words to another sequence of words (in some other language).  Note that in the original [paper](https://arxiv.org/pdf/1409.3215.pdf) written by Ilya et al., the sequence to sequence learning could be a quite generalizable concept, below are several examples of sequences:

<img style="display: block;" class="img-fluid" src="https://i.imgur.com/ZVfrUta.png" alt="seq2seq.">
<p class="small">"Examples of sequences" by seq2seq ICML 17' tutorial</p>

In my opinion, the motivation for developing the modern sequence to sequence learning methods is driven by the following two problems:

* The traditional ML methods rely on the consistent input dimensions. That is to say, the model only deals with the inputs that share a constant shape.

* Given the training pairs (each pair contains two sequences usually sampled from two domains), the task is essentially to match the distribution of the former to the latter. How do we model, evaluate or optimize the process?

The first problem drives the development of recurrent neural network (RNN), while the second problem drives the architecture design of the moderm seq2seq learning models. We now briefly introduce the three important models to support the further discussion, they are RNN, LSTM, and self-attention.

### Recurrent Neural Network and Long Short Term Memory Network

We would give a short introduction of the model structures and optimization techniques, the main focus of this section remains to be the discussion in the context of sequence-to-sequence learning.

In short, Recurrent Neural Network (RNN) is a kind of NN architectures in which the model computation graph is directed cyclic graph, while the Long Short Term Memory (LSTM) Network is a more complex version of the core network of RNN. Though sound intuitive, RNNs are proven to be powerful in many ways, e.g., [RNNs are Turing Complete](https://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf), [RNNs are nearly intelligence-equivalent by approaching the asymptotic limit in text compression](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf). To make the following introduction non-trivial, we consider the predicting-next-character task, where the model is provided a sequence of characters and is required to predict the next character in the end of the sequence. (Note: The discussion here are mainly derived from the Ph.D [thesis](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf) of Ilya Sutskever).

**RNN Formulation**:

There are two perspectives on RNN, view it as a single network whose computation graph is cyclic, or view it as a super deep neural network with shared weights among the blocks (unrolled view). 

<img style="display: block;" class="img-fluid" src="https://i.imgur.com/ELw9Iu9.png" alt="RNN.">
<p class="small">"An unrolled RNN" by colah's blog</p>

My apology for the inconsistency, but we are going to use a slightly different notation from the figure. Let $i_t$ be the input at t-th moment, and $o_t$ be the output at t-th moment. Aparently RNN utilizes a hidden representation of the data, denoted by $h_t$. Now we define the input-output map as:

$$
o_t = g(h^o_t) \\
h^o_t = W_{oh}h_t + b_o \\
h_t = f(h^i_t) \\
h^i_t = W_{hi}i_t + W_{hh}h_{t-1} + b_h
$$

If you are familiar with the feed forward network, these are just two dense layers connected by the corresponding activation functions. The only difference is that the input layer takes the last moment's state as part of the input, i.e., a combination of the input and the hidden state. 

Before we discuss the LSTM network, we take a glance at the ackpropagation through time algorithm (BPTT). We first define the training loss function of RNN as the cumulative loss over time:

$$
\mathcal{L}(o, y) = \sum_{t=1}^T \mathcal{l}_t(o_t, y_t)
$$

Let's calculate the partial derivative of the loss over $W_{hi}$ and $W_{hh}$, since the calculation w.r.t $W_{oh}$ is straight forward. We have the following:

$$
\frac{\partial \mathcal{L}}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hh}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hh}} \\
\frac{\partial \mathcal{L}}{\partial W_{hi}} = \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial W_{hi}} =  \sum_{t=1}^T \frac{\partial \mathcal{l}_t}{\partial o_t}\frac{\partial o_t}{\partial h^o_t}W_{oh}\frac{\partial h_t}{\partial h^i_t}\frac{\partial h^i_t}{\partial W_{hi}}
$$

Note here you cannot directly calculate that $\frac{\partial h^i_t}{\partial W_{hh}}=h_{t-1}$ since $h_{t-1} = h_{t-1}(\cdot, W_{hh})$, that's because $W_{hh}$ is shared across the whole sequence. Instead, we apply the chain rule on the partial derivative of the multiplication $W_{hh}h^{t-1}$, we have:

$$
\frac{\partial h_t}{\partial W_{hh}}=\frac{\partial h_t}{\partial h^i_t}(h_{t-1}+W_{hh}\frac{\partial h_{t-1}}{\partial W_{hh}})=\frac{\partial h_t}{\partial h^i_t}(h_{t-1}+W_{hh}(\frac{\partial h_{t-1}}{\partial h^i_{t-1}}(h_{t-2}+W_{hh}\frac{\partial h_{t-2}}{\partial W_{hh}}))))= ...
$$
Following the chain rule, we obtain the following:
$$
\frac{\partial h_t}{\partial W_{hh}} = \sum_{j=0}^{t-1} \left(\prod_{k=0}^{j}\frac{\partial h_{t-k}}{\partial h^i_{t-k}}W_{hh}\right)h_{t-j-1}
$$
The above computation intuitively not stable, since the high order matrix multiplication is introduced. The resulted training process would be in face of potential gradient explosion and gradient vanishing. A straight forward solution is to truncate the historical computation with a relatively short window, and $j$ in the formula would start from $t-w$, where $w$ is the window size. Apparently we lose the long-term learning ability immediately, and that's where the LSTM network helps. You may have noticed that one of the most apparent problems of vanila RNN is that it encodes all the historical (and any potential future) information into one matrix, which may align with the intuition of our observations on the humans, yet failed to be effectively trained with gradient descent. 
> Note: You can try to enhance RNN to mitigate these problems, sometimes it's just a problem of imagination.

LSTM network tries to solve this problem with an architecture that splits the knowledge representation into two parts: The hidden state $h$ and the memory $C$. The good thing about $C$ is $\frac{\partial C_t}{\partial C_{t-1}}=1$. How would this help? Let's look at RNN and BPTT in an abstract view. Say, we regard the recurrent network as extracting a knowledge representation $W$ from data. So what we need to do is to define an interface of the knowledge representation $h$, a data to interface transformation $i$, and a interface to data transformation $o$. So the learning process could be defined as:
$$
\min \mathbb{E}_{x,y\sim P(x,y)}[\mathcal{L}(o(x), y)]\\
o(x)=o(h(i(x),W))
$$
This looks like any ML objectives, for the data structure of sequences, we model the data as $x=[x_1, x_2, ...x_T]$, then for RNN, we could process the data segment by segment. Now $i(x)$ is just any neural layer operation $\phi_i (W_ix+b_i)$. The point is that we model $h$ as $h = [h_1, h_2, ... h_t]$, while $h_t=W_{hh}h_{t-1}+W_{ih}x_t+b_h=W[h_{t-1}, x_t, 1]$, and $o=\phi_o(h)$, then we get the formulation of RNN. The gradient calculation follows:
$$
\frac{\partial \mathcal{L}}{\partial W} = \frac{1}{T}\sum_{t=1}^T \frac{\partial \mathcal{L_t}}{\partial W} = \frac{1}{T}\sum_{t=1}^T \frac{\partial \mathcal{L_t}}{\partial o_t}\frac{\partial o_t}{\partial h_t}\sum_{k=1}^t \prod_{j=k+1}^t\left(\frac{\partial h_j}{\partial h_{j-1}}\right)\frac{\partial h_k}{\partial W}
$$
["On the difficulty of training recurrent neural networks"](https://proceedings.mlr.press/v28/pascanu13.html) pointed out that, for some constant $\gamma$:
$$
\prod_{j=k+1}^t\frac{\partial h_j}{\partial h_{j-1}} = \prod_{j=k+1}^t diag(\frac{\partial h_j}{\partial h^i_{j-1}})\frac{\partial h^i_{j-1}}{\partial h_{j-1}}\approx 
\left\{ \begin{array}{rcl}
& 0\quad \text{for }max(eigen(diag))<\frac{1}{\gamma} \\
& \infty\quad \text{for }max(eigen(diag))>\frac{1}{\gamma}
\end{array}\right.
$$

 The interesting part is that now we can see that the forward calculation already involved with the $W_{hh}^t$. Of course if we model $i(x)$ as a direct map from $x$ to $h$, the problem is solved, but imagin how big the space of $x$ is, for example $x$ represents an arbitrary sentence written in English. So for now we still model $x$ as a sequence decomposition $[x_1, x_2, ...x_T]$, and see if there are better modelling for $h$. Now let's take a look at LSTM network.

<img style="display: block;" class="img-fluid" src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="LSTM.">
<p class="small">"LSTM network" by colah's blog</p>

This needs a few explanations, so the upper horizontal line in the figure denotes the memory $C$ and the lower horizontal line denotes the hidden state $h$. The calculation follows (from the left vertical line to the right):

$$
A_t = \sigma(W_A[h_{t-1},x_t]+b_A)\\
B_t = \sigma(W_B[h_{t-1},x_t]+b_B)\\
\hat{C}_t=tanh(W_C[h_{t-1},x_t]+b_C)\\
C_t = A_t\odot C_{t-1}+B_t\odot \hat{C}_t\\
D_t = \sigma(W_D[h_{t-1},x_t]+b_D)\\
h_t = D_t\odot tanh(C_t)
$$

where $\odot$ denotes the hadamard product $A\odot B_{ij}=A_{ij}B_{ij}$. The terminology "gate" in most of the introduction of LSTM denotes the sigmoid activation, since the resulted vector would serve as a mask with values near 0 or near 1. In the original form of LSTM network, $A_t$ is set to be $A_{i}=1, \forall i$, thus we have $\frac{\partial C_t}{\partial C_{t-1}}=I$. Here the variant uses a learnt gate (called "forget gate") to control the memory, the gate would clear some of the values in the memory. So we can initialize $b_A$ as near 1 and then the network is trained to construct forget gate. Now the architecture seems a little bit complicate, we can use the gated recurrent unit (GRU) to simplify the process (practically, GRU saves memory usage):

$$
A_t = \sigma(W_A[h_{t-1},x_t]+b_A)\\
B_t = \sigma(W_B[h_{t-1},x_t]+b_B)\\
C_t=tanh(W_C[A_t\odot h_{t-1},x_t]+b_C)\\
h_t = (\mathbf{1}-B_t)\odot h_{t-1} + B_t\odot C_t
$$

Now we've come to a critical point, before we go any further, think for a minute: how can we improve those models in every possible ways? 

Please check the two examples in [pytorch turorial: Classifying names via RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) and [pytorch tutorial: Generating names via RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html), these two materials are sufficient (at least for me) for understanding the basic implementation of RNNs. 

### Encoder-Decoder Architecture

Recall that our goal is to model the sequence prediction problem, ["Sequence to Sequence Learning
with Neural Networks"](https://arxiv.org/pdf/1409.3215.pdf) proposes the sequence-to-sequence learning with LSTM network. The very fundamental idea of solving this problem is the encoder-decoder architecture. Recall that the RNN would produce the hidden state at every time step, when a sentence is feed into RNN, every word in the sentence represents the input at every time step. We recognize the final hidden state as the context vector, being an abstract representation of the input sentence. Now you can imagine how another RNN take the context vector as part of the input to decode the information from this representation. The diagram illustration is:


<img style="display: block;" class="img-fluid" src="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/b3cd54c72cd6e4e63f672d334c795b4fe744ef92//assets/seq2seq1.png" alt="Seq-to-seq architecture">
<p class="small">"sequence to sequence RNNs" by pytorch-seq2seq tutorial</p>

The above example shows the process of machine traslation. In the decoder, we reuse the decoded output at $t-1$ as the input at $t$. This is called autoregressive model, where the overall goal is to predict the next element in the sequence with all the previous predictions. Formally, the autoregressive model is a probabilistic model $p(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)})$ such that:

$$
p\left(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)}\right)=\prod_{t=1}^T p\left(\mathbf{x}^{(t)} \mid \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t-1)}\right)
$$

The network usually uses the softmax activation for predicting the distribution (assuming the discrete distribution). In MT task, the decoder would output the translated sequence one word by one word, the previous predicted words would be feed into the decoder as part of the input.

The reader who gets confused may refer to this [tutorial](https://github.com/bentrevett/pytorch-seq2seq) for a step-by-step sequence to sequence modeling process, and it's written much better than my post.

### Self-Attention and Transformer

You may noticed that the context vector serves as the information bottleneck (IB) of the encoder-decoder architecture. The IB would compress and preserve the useful representation of the original input, this aim is, however, not assured to be accomplished. Since the context vector is merely the final hidden state of the RNN encoder, it is natural to think that the context vector would somehow lose information from the faraway input tokens. Now if we stack all the hidden states into one big tensor, apparently there would be redundance of information. We need a selection of the hidden states to preserve the important information, with respect to what? The simple solution is to let the decoder choose. We now introduce the attention mechanism. This mechanism still starts from a simple idea: we cannot take all the hidden states as the context vector, we can sum over them to create one context vector. The summation is weighted by the decoder. So for each decoder step, we regenerate the context vector, to focus on the information that is relavant to the current prediction. Still, a simple idea is to compute a relavance score between the last hidden state of the current decoder input and each encoder hidden state output:

$$
 \operatorname{score}\left(\mathbf{h}_{i-1}^d, \mathbf{h}_j^e\right)=\mathbf{h}_{i-1}^d\cdot\mathbf{h}_j^e
$$

 This similarity is then used to compute weights:

$$
\alpha_{i j} =\operatorname{softmax}\left(\operatorname{score}\left(\mathbf{h}_{i-1}^d, \mathbf{h}_j^e\right)\right) =\frac{\exp \left(\operatorname{score}\left(\mathbf{h}_{i-1}^d, \mathbf{h}_j^e\right)\right.}{\sum_k \exp \left(\operatorname{score}\left(\mathbf{h}_{i-1}^d, \mathbf{h}_k^e\right)\right)}
$$

For the discussion of transformers in this post, we mainly focus on building the connection between this architecture and the NLP problems. Several facts need to be clarified:

* We discuss the transformers in solving the problem of causal language modeling, which means to predict the next word in the sentence in an autoregressive fashion.

* Transformers are not built with recurrent blocks, so it's different from the way we build RNN (LSTM) blocks. Instead, transformers use multiple multi-head self-attention blocks to project the al input to a series of hidden states of the same length as the input, and finally obtain a highly abstracted representation.

* Transformer's key novelty is the self-attention mechanism, which allows the transformer to build layer-by-layer information aggregation.
  
Self-attention appears in two ways: causal (backward looking) self-attention and the bidirectional self-attention. The former assumes the causal structure in the sequence where only the historical information is accessable, while the latter could access all the information in the sequence. In this post, we mainly focus on the causal setting, since it fits in the intuition of natural language directly. As we could conclude from the original attention mechanism, the only aim is to computing a relavance score between the current hidden state and the historical hidden states, the dot product similarity may be too naive. The self-attention introduced a role-based encoding scheme:

* When the embedding is in the role of the current focus, we refer to it as a **query**.
* When the embedding is in the role of a preceding input, we refer to it as a **key**.
* When it serves as a **key**, we also compute a **value** to serve as the embedding to be weighed. So the final aggregation is calculated based on **value** instead of the embedding **x** itself.

The self-attention is calculated as:

$$
\begin{aligned}
\mathbf{q}_i=\mathbf{x}_i \mathbf{W}^{\mathbf{Q}} ; \mathbf{k}_i & =\mathbf{x}_i \mathbf{W}^{\mathbf{k}} ; \mathbf{v}_i=\mathbf{x}_i \mathbf{W}^{\mathbf{v}} \\
\operatorname{score}\left(\mathbf{x}_i, \mathbf{x}_j\right) & =\frac{\mathbf{q}_i \cdot \mathbf{k}_j}{\sqrt{d_k}} \\
\alpha_{i j} & =\operatorname{softmax}\left(\operatorname{score}\left(\mathbf{x}_i, \mathbf{x}_j\right)\right) \forall j \leq i \\
\mathbf{a}_i & =\sum_{j \leq i} \alpha_{i j} \mathbf{v}_j
\end{aligned}
$$
  
It's clear now, we transform the original embedding into three different vector spaces, and produce the final output to the next layer in the vector space of **value**. Here $d_k$ is the dimension of the vector space spanned by **key**, the division by $\sqrt{d_k}$ is to maintain the numerical stability of the dot product ($q_i \sim \mathcal{N}\left(0, \sigma^2\right), k_i \sim \mathcal{N}\left(0, \sigma^2\right) \rightarrow \operatorname{Var}\left(\sum_{i=1}^{d_k} q_i \cdot k_i\right)=\sigma^4 \cdot d_k$). Another advantage is that for a sequence constitutes of multiple tokens, the self-attention could be effectively calculated by stacking the embeddings of every token:

$$
\begin{aligned}
\mathbf{Q}=\mathbf{X} \mathbf{W}^{\mathbf{Q}} ; \mathbf{K} & =\mathbf{X} \mathbf{W}^{\mathbf{K}} ; \mathbf{V}=\mathbf{X} \mathbf{W}^{\mathbf{V}} \\
\mathbf{A} & =\operatorname{softmax}(\frac{\mathbf{Q} \cdot \mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}
\end{aligned}
$$

For the causal self-attention, we need to mask the upper triangular of the matrix $\mathbf{Q} \cdot \mathbf{K}^T$, corresponding to the future **keys**. This matrix also reveals the quadratic relationship between the computation scale and the input sequence length.

The above description is referred to as the single head self-attention, the building block of the multi-head self-attention, which is what transformer used in practice. The multi-head self-attention adopts a group of the single-head versions, which means that we calculate $h$ different self-attention results with $\mathbf{W}_i^{\mathbf{Q}}$, $\mathbf{W}_i^{\mathbf{k}}$, and $\mathbf{W}_i^{\mathbf{V}}$, for $i\in[h]$. We aggregate the results with a linear projection $\mathbf{W}^\mathbf{O}$ to produce the output:

$$
\begin{aligned}
\mathbf{Q}_i=\mathbf{X} \mathbf{W}_i^{\mathbf{Q}} ; \mathbf{K}_i & =\mathbf{X} \mathbf{W}_i^{\mathbf{K}} ; \mathbf{V}_i=\mathbf{X} \mathbf{W}_i^{\mathbf{V}} \\
head_i & =\operatorname{softmax}(\frac{\mathbf{Q}_i \cdot \mathbf{K}_i^T}{\sqrt{d_k}})\mathbf{V}_i \\
\mathbf{A}=\text { MultiHeadAttention }(\mathbf{X})&=\left(\text { head }_1 \oplus \text { head }_2 \ldots \oplus \text { head }_h\right) \mathbf{W}^O
\end{aligned}
$$

where the $\oplus$ denotes the concatenation, also note that we use an output matrix $\mathbf{W}^O$ to recover the input dimension $d$ from the hidden dimension $d_k$. Now as illustrated in the figures of transformer architecture, the weighted sum is then passed to a feed-forward network (assuming 1 hidden layer) with two layer normalization operations:

$$
\begin{aligned} 
\mathbf{U}^{''} & =\operatorname{LayerNorm}\left(\mathbf{X}+\mathbf{A} ; \gamma_1, \beta_1\right) \\ 
\mathbf{U}^{'} & =\mathbf{W}_2^T \operatorname{ReLU}\left(\mathbf{W}_1^T \mathbf{U^{''}}\right) \\
\mathbf{U} & =\operatorname{LayerNorm}\left(\mathbf{U}^{''}+\mathbf{U}^{'} ; \gamma_2, \beta_2\right)\end{aligned}
$$

The layer normalization function is defined as:

$$
\begin{aligned} & \operatorname{LayerNorm}(\mathbf{z} ; \gamma, \beta)=\gamma \frac{\left(\mathbf{z}-\mu_{\mathbf{z}}\right)}{\sigma_{\mathbf{z}}}+\beta, \quad \mathbf{z}, \gamma, \beta\in \mathbb{R}^k\\ & \mu_{\mathbf{z}}=\frac{1}{k} \sum_{i=1}^k \mathbf{z}_i, \quad \sigma_{\mathbf{z}}=\sqrt{\frac{1}{k} \sum_{i=1}^k\left(\mathbf{z}_i-\mu_{\mathbf{z}}\right)^2 .}\end{aligned}
$$

So what about the transformers? Now we know that the self-attention block(s) transformed the input sequence to another space where the embeddings denote the attention to the other locations in the sequence. Observe that in this process we have the parameter set 

$$
\theta=\{\mathbf{W}^{Q}, \mathbf{W}^{K}, \mathbf{W}^{V}, \mathbf{W}^{O}, \mathbf{W}_{1}, \mathbf{W}_2, \gamma_1, \beta_1, \gamma_2, \beta_2\}
$$

We use $f_{\theta}(\mathbf{x})$ to denote this block, the transformer is the composition of $L$ blocks: $f_{\theta_L} \circ \cdots \circ f_{\theta_1}(\mathrm{x}) \in \mathbb{R}^{n \times d}$. Recall that the hyperparameters are the embedding dim $d$, the hidden dim $d_k$, the feed-forward network width $m$, the number of heads $h$, and the number of blocks $L$, commonly we set $d=512, d_k=64, m=64, h=8$, the original paper set $L=6$, this number has been expanded largely. 

Now if we transfer from the RNN families to the transformer families, you may notice that the sequence learning property is lost! Yes, the transformer does not assume any specific structure on the input embeddings set, if you want to learn the sequential structure in the token sequence, you must encode the structure into the embeddings. Now consider the simplest way of introducing the sequential structure embedding: $x=x\oplus p$, where $p$ is an indicator vector $e_i$ with $i$ indicating the $x$'s position in the sequence. The original paper of transformer propose that:

$$
\mathbf{p}_{k, 2 i}=\sin \left(\frac{k}{10000^{2 i / d}}\right), \quad \mathbf{p}_{k, 2 i+1}=\cos \left(\frac{k}{10000^{2 i / d}}\right)
$$

the positional embedding is then added directly to the input.

# Large Language Model Foundations

## Recap on Transformer Block

### Encoder-Decoder Architecture

Given a source sequence $x=(x_1, x_2,... x_m)$, a target sequence $y=(y_1, y_2,..., y_n)$.

* A transformer encoder outputs a sequence of hidden states:
  $$
  E(x) = H = FFN_{block}(Attn_{block}(x))
  $$
  The two blocks are wrapped with layer normalization and residual connection respectively:
  $$
  \begin{aligned}
  Attn_{block} &= LNorm(Att_{self}(x)+x) \\
  FFN_{block} &= LNorm(FFN(Attn_{block}(x))+Attn_{block}(x))
  \end{aligned}
  $$
  Then we can stack $E(x)$ vertically to make the network deeper, if there were $L$ layers of $E(x)$, the output $H^L$ is the output of transformer encoder. 

* A transformer decoder accepts the hidden states together with the previous generated target sequence $y= (y_1, y_2,... y_{n-1})$, and output the decoded sequence:
  $$
  \begin{aligned}
  D(y_{0:n-1}, H^L) = FFN_{block}(Attn_{cross}(Attn_{block}(y_{0:n-1})))
  \end{aligned}
  $$
   The difference lies on the cross attention block, where we just replace the query with the target sequence, and use key/value of the hidden states. It is important to notice that in order to generate the sequence correctly, we need to apply a causal mask when calculating the attention weights, something like $Attn^{dec}_{self} = softmax(QK^T/\sqrt{d})\odot M\cdot V$. The causal mask is just a lower-triangular matrix.

  We can also stack the decoding blocks to make it deeper, $L$ stacked decoder layers would give us the final output of transformer decoder $S^L$. Eventually to make the output a prob distribution on the target vocabulary, we use a linear transformation $O=softmax(S^L\cdot W)$.

### Self-Attention

The self-attention mechanism is something like a weighted average, with weighting matrix be calculated according to the input sequence. In fact, the original self attention mechanism is just a scaled dot product operation:
$$
h_i = \sum_{j}softmax(h_i\cdot h_j/\sqrt{d})\cdot h_j
$$
This formulation is then generalized to the famous $QKV$ attention, where the $h$ is replaced by:
$$
\begin{aligned}
h^q &= h\cdot W^q \\
h^k &= h\cdot W^k \\
h^v &= h\cdot W^v \\
Attn_{self}(h) &= softmax(H^q\cdot H^{k}/\sqrt{d})\cdot H^v
\end{aligned}
$$
Such kind of asymmetric projection gives attention block the ability of recognizing the asymmetric relationship, like an undirected graph upgrading to a directed graph. The multi-head attention mechanism is then applied as:
$$
MHAttn_{self}(h) = concat(Attn^1_{self}(h), ..., Attn^{N_{head}}(h))\cdot W^{c}
$$

### Positional Encoding



### Layer Normalization

The layer normalization is a linear transformation on the normalized vectors:
$$
\begin{aligned}
\operatorname{LNorm}(\mathbf{h})&=\mathbf{g} \odot \frac{\mathbf{h}-\mu}{\sigma+\epsilon}+\mathbf{b} \\
\mu & =\frac{1}{d} \cdot \sum_{k=1}^d h_k \\
\sigma & =\sqrt{\frac{1}{d} \cdot \sum_{k=1}^d\left(h_k-\mu\right)^2}
\end{aligned}
$$
There are post-norm and pre-norm, differing in the order of arranging the LNorm, operation function, and the residual connection. Let $LN$ be the LNorm and $F$ be the operation:

* Pre-norm: $LN(F(x)+x)$
* Post-norm: $x+F(LN(x))$

The origin transformer architecture leverage the post normalization.

### Complexity Analysis of Transformer

| Sub-model   |                            | # of Parameters                                 | Time Complexity                                  | $\times$ |
| :---------- | :------------------------- | :---------------------------------------------- | :----------------------------------------------- | :------- |
| **Encoder** | Multi-head Self-attention  | $4 d^2$                                         | $O\left(m^2 \cdot d\right)$                      | L        |
|             | Feed-forward Network       | $2 d \cdot d_{\mathrm{ffn}}+d+d_{\mathrm{ffn}}$ | $O\left(m \cdot d \cdot d_{\text {ffn }}\right)$ | $L$      |
|             | Layer Normalization        | $2 d$                                           | $O(d)$                                           | $2 L$    |
| **Decoder** | Multi-head Self-attention  | $4 d^2$                                         | $O\left(n^2 \cdot d\right)$                      | $L$      |
|             | Multi-head Cross-attention | $4 d^2$                                         | $O(m \cdot n \cdot d)$                           | $L$      |
|             | Feed-forward Network       | $2 d \cdot d_{\mathrm{ffn}}+d+d_{\mathrm{ffn}}$ | $O\left(n \cdot d \cdot d_{\text {ff }}\right)$  | $L$      |
|             | Layer Normalization        | $2 d$                                           | $O(d)$                                           | $3 L$    |

 $m=$ source-sequence length, $n=$ target-sequence length, $d=$ default number of dimensions of a hidden layer, $d_{\mathrm{ffn}}=$ number of dimensions of the FFN hidden layer, $\tau=$ number of heads in the attention models, and $L=$ number of encoding or decoding layers. The column $\times$ means the number of times a sub-model is applied on the encoder or decoder side. The time complexities are estimated by counting the number of multiplication of floating-point numbers.

### Techniques for Training

1. Learning rate scheduling: Here lists some of the recommended practices:
   1. Warm up then decay: $l r=l r_0 \cdot \min \left\{n_{\text {step }}^{-0.5}, n_{\text {step }} \cdot\left(n_{\text {warmup }}\right)^{-1.5}\right\}$

### Architectures

In the standard sequence to sequence language modeling scenarios, like machine translation, we leverage the encoder-decoder architecture to relate the source language sentence and the target language sentence. However, there are other two architectures of language models, namely decoder only and encoder only.

* Decoder only: Decoder only pretraining is based on simple scheme, we just drop the cross attention layer, and perform the language generation task, where we let the decoder generate the sentence's next token iteratively. The loss function could be the cross entropy on the candidates.

* Encoder only: The encoder only architectures are used in several scenarios, especially the downstream task where we need a encoded representation of the natural language objects, the encoded hidden states serve for the purpose. The encoder pre-training is relatively hard, since we don´t have the gold standard for the good state representation. There are mainly two methods:

  1. Supervised pretraining: the supervised pretraning use some downstream tasks to construct the (sometimes multi-objective) loss function for the encoder itself. It may require large amount of labeled corpus in a wide range of downstream tasks, but once the model is pretrained, finetuning it for certain task become relatively easy.
  2. Unsupervised pretraining: The unsupervised pretraining usually leverage the scheme of masked language modeling, i.e., mask the input sequence on some positions, then try to reconstruct the masked words. 
     1. Masked language modeling: Directly mask the input and train encoder only, differing from the decoder causal mask, this approach would try to predict the masked words with the information of the both left and right positions. 
     2. Permuted language modeling: To mitigate the problem of losing masked words correlations between each other, the permuted language modeling is proposed as a surrogate. Here we consider the standard encoder-decoder training procedures, by manipulating the causal mask, we could  force the decoder to generate token with respect to any positions of a context, so we don´t need to mask the input sequence anymore, but permute the causal mask multiple times to train the encoder.
     3. Mixed approach: we could construct more training task based on the masked language modeling and self-supervised learning methods. For example, ELECTRA proposed that use a masked language model to generate the masked token (this encoder could be trained independently), then use the masked token replace the original token in the sentence, then train another encoder to classify if the token has been replaced (also trained independently). Then we use the classifier encoder after training.

* Encoder-Decoder: The encoder-decoder architecture has more general abilities, in T5 model, the authors frame many different language tasks as sequence-to-sequence task: Source Text $\rightarrow$ Target Text. So for any task, if you can frame it into the above form, it could be modeled with encoder-decoder architecture. The simplest way could be integrate your problem instruction and the problem input into the source text, such as:

  [CLS] Translate from Chinese to English: 你好! $\rightarrow$ 〈s〉 Hello!

  1. BERT-style pretraining: The BERT presents the training scheme of masked sentence prediction, where we mask the input sentence as what we did in the masked encoder training, then the decoder tries to recover the whole sentence (in causal order), the loss however would just be applied in the mask positions.
  2. Denoising auto-encoder pretraining: If we let the loss cover the whole generated sentence, i.e., correctly recover the masked tokens while generating the same unmasked tokens as the input. However, the essence of denoising training could be generalized into much more schemes, including token masking, token deletion, span masking, sentence reordering, document rotation, etc., The performance of encoder-decoder architecture depends heavily on the training scheme, choose the correct one with sufficient experiments.

| Category                | Method                   | Enc       | Dec       | E-D       | Input                                                        | Output                                                       |
| :---------------------- | :----------------------- | :-------- | :-------- | :-------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| Language Modeling       | Causal LM                |           | $\bullet$ | $\bullet$ |                                                              | The ${ }^1$ kitten ${ }^2$ is ${ }^3$ chasing ${ }^4$ the ${ }^5$ ball ${ }^6 .^7$ |
|                         | Prefix LM                |           | -         | -         | [C] The kitten is                                            | chasing ${ }^1$ the ${ }^2$ ball ${ }^3$. ${ }^4$            |
| Masked LM               | Masked LM                | $\bullet$ |           | $\bullet$ | $[\mathrm{C}]$ The kitten [M] chasing the [M].               | _ _ ${ }^{\text {is }}$ _ _ ball _                           |
|                         | MASS-style               | -         |           | -         | $[\mathrm{C}]$ The kitten $[\mathrm{M}][\mathrm{M}][\mathrm{M}]$ ball . | _ _ is chasing the _ _                                       |
|                         | BERT-style               | $\bullet$ |           | $\bullet$ | $[\mathrm{C}]$ The kitten [M] playing the [M].               | _ kitten is chasing _ ball _                                 |
| Permuted LM             | Permuted LM              | -         |           |           | [C] The kitten is chasing the ball .                         | The ${ }^5$ kitten ${ }^7$ is ${ }^6$ chasing ${ }^1$ the ${ }^4$ ball ${ }^2 .^3$ |
| Discriminative Training | Next Sentence Prediction | $\bullet$ |           |           | [C] The kitten is chasing the ball . Birds eat worms .       | $\operatorname{Pr}($ IsNext $\mid$ representation-of-[C])    |
|                         | Sentence Comparison      | -         |           |           | Encode a sentence as $\mathbf{h}_a$ and another sentence as $\mathbf{h}_b$ | Score( $\mathbf{h}_a, \mathbf{h}_b$ )                        |
|                         | Token Classification     | $\bullet$ |           |           | [C] The kitten is chasing the ball .                         | $\operatorname{Pr}(\cdot \mid$ The $) \operatorname{Pr}(\cdot \mid$ kitten $) \ldots \operatorname{Pr}(\cdot \mid \cdot)$ |
| Denoising Auto-encoding | Token Reordering         |           |           | $\bullet$ | [C] . kitten the chasing The is ball                         | The ${ }^1$ kitten ${ }^2$ is ${ }^3$ chasing ${ }^4$ the ${ }^5$ ball ${ }^6 .^7$ |
|                         | Token Deletion           |           |           | $\bullet$ | [C] The kitten is ehasing the ball .                         | The ${ }^1$ kitten ${ }^2$ is ${ }^3$ chasing ${ }^4$ the ${ }^5$ ball ${ }^6 .^7$ |
|                         | Span Masking             |           |           | $\bullet$ | $[\mathrm{C}]$ The kitten $[\mathrm{M}]$ is $[\mathrm{M}]$.  | The ${ }^1$ kitten ${ }^2$ is ${ }^3$ chasing ${ }^4$ the ${ }^5$ ball ${ }^6 .^7$ |
|                         | Sentinel Masking         |           |           | $\bullet$ | $[\mathrm{C}]$ The kitten [X] the [Y]                        | $[\mathrm{X}]^1$ is ${ }^2$ chasing ${ }^3[\mathrm{Y}]^4$ ball ${ }^5 .^6$ |
|                         | Sentence Reordering      |           |           | $\bullet$ | [C] The ball rolls away swiftly . The kitten is chasing the ball . | The ${ }^1$ kitten ${ }^2$ is ${ }^3$ chasing ${ }^4$ the ${ }^5$ ball ${ }^6 .^7$ The $^8$ ball ${ }^9$ rolls ${ }^{10}$ away ${ }^{11}$ swiftly ${ }^{12}$. ${ }^{13}$ |
|                         | Document Rotation        |           |           | $\bullet$ | [C] chasing the ball . The ball rolls away swiftly. The kitten is | The ${ }^1$ kitten ${ }^2$ is ${ }^3$ chasing ${ }^4$ the ${ }^5$ ball ${ }^6 .^7$ The $^8$ ball $^9$ rolls $^{10}$ away $^{11}$ swiftly ${ }^{12} .{ }^{13}$ |

## Large Language Models: Overview

In this section and the following sections, we refer to LLM as decoder-only transformer architecture. LLM is famous for not only the excellent performance, but also the incredibly large size, the following table shows the parameter size of the several main stream models:

| LLM                             | # of Parameters | Depth $L$ | Width $d$ | # of Heads (Q/KV) |
| :------------------------------ | :-------------- | :-------- | :-------- | :---------------- |
| GPT-1 [Radford et al., 2018]    | 0.117 B         | 12        | 768       | 12/12             |
| GPT-2 [Radford et al., 2019]    | 1.5 B           | 48        | 1,600     | 25/25             |
| GPT-3 [Brown et al., 2020]      | 175B            | 96        | 12,288    | 96/96             |
| LLaMA2 [Touvron et al., 2023b]  | 7B              | 32        | 4,096     | 32/32             |
|                                 | 13B             | 40        | 5,120     | 40/40             |
|                                 | 70B             | 80        | 8,192     | 64/64             |
| LLaMA3/3.1 [Dubey et al., 2024] | 8B              | 32        | 4,096     | 32/8              |
|                                 | 70B             | 80        | 8,192     | 64/8              |
|                                 | 405B            | 126       | 16,384    | 128/8             |
| Gemma2 [Team et al., 2024]      | 2B              | 26        | 2,304     | 8/4               |
|                                 | 9B              | 42        | 3,584     | 16/8              |
|                                 | 37B             | 46        | 4,608     | 32/16             |
| Qwen2.5 [Yang et al., 2024]     | 0.5 B           | 24        | 896       | 14/2              |
|                                 | 7B              | 28        | 3,584     | 28/4              |
|                                 | 72B             | 80        | 8,192     | 64/8              |
| DeepSeek-V3 [Liu et al., 2024a] | 671B            | 61        | 7,168     | 128/128           |
| Falcon [Penedo et al., 2023]    | 7B              | 32        | 4,544     | 71/71             |
|                                 | 40B             | 60        | 8,192     | 128/128           |
|                                 | 180B            | 80        | 14,848    | 232/232           |
| Mistral [Jiang et al., 2023a]   | 7B              | 32        | 4,096     | 32/32             |

LLMs with such amount of parameters would require even larger size of training dataset, more complex training paradigm, distributed training/inference framework, and certain optimization techniques in order to use them in practice. The following table summarizes the topics we are going to discuss in the later sections:

| Topic            | Technique                    | TL,DR |
| ---------------- | ---------------------------- | ----- |
| Model Families   | BERT Series                  |       |
|                  | T5                           |       |
|                  | GPT Series                   |       |
|                  | Other Architectures          |       |
| Pre-Training     | Unsupervised Pre-training    |       |
|                  | Supervised Pre-training      |       |
|                  | Semi-supervised Pre-training |       |
| Finetuing        | Instruction Tuning           |       |
|                  | Alignment Tuning             |       |
| Data Preparation | Quality Filtering            |       |
|                  | Privacy Reduction            |       |
|                  | Tokenization                 |       |

### Model Families

#### BERT Series

BERT (Birectional Encoder Representations from Transformers) is the representative encoder only model, with three main features: 1) Stacked transformer encoders with an output head customized for various downstream tasks. 2) Integrating masked language modeling (MLM) loss and the next sentence prediction loss. 3)  110M and 340M parameters, trained on 137B tokens from BooksCorpus and English Wikipedia. Specifically, BERT uses post LNorm within each layer. GELU (Gaussian error linear unit) for activation function, smoother than ReLU, softmax for attention weights calculation, and linear transformation in the final output layer for MLM task. BERT uses 15% randomly masked tokens for MLM loss, among the masked tokens, 80% of them are replaced with [MASK], 10% of them are replaced with random tokens, and 10% of them are unchanged (encouraging the model to solve the problem with easy way). The next sentence prediction loss is calculated by the binary classification of whether two sentences are consecutive. BERT uses AdamW for optimizer, with $\beta_1=0.9, \beta_2=0.99,\epsilon=10^{-6}$. The training strategy contains: linear warmup in frist 10k tokens, then linear decay, 256-1024 sequence for a batch, and 1e-4 ~ 1e-5 for learning rate. Finetuning with smaller learning rate, and 2-4 epochs with full sequence length is recommended. 

RoBERTa is a popular variant of BERT, it enhances BERT with 10x training data, removing the next sentence prediction task, 32x mini-batch size, larger learning rate, dynamic masking each epoch. RoBERTa also uses Byte-level BPE for text encoding, which aligns with the GPT-2. ALBERT reduces the BERT parameter size with embedding matrix split and repeating layers split among groups. DeBERTa introduced the disentangled content-and-relative-position attention mechanism in encoder and a decoder with absolute position to predict the masked token. ELECTRA introduces the replaced token detection for improving efficiency in contrast with MLM, as we illustrated above, the RTD computes the loss on the whole input token sequence rather than the masked positions.

#### T5 Series

T5 stands for the Text-To-Text Transfer Transformer, the developers of T4 sketched a blueprint where all the NLP tasks are framed as text-to-text form, so could be tackled with one unified base model. T5 is an encoder-decoder transformer, the encoder attention block allows for every input token attending to all the input tokens, the cross attention block allows for every target token attending to all the input tokens, the decoder self attention block adopts the upper triangular causal attention mask. T5 is symmetric, i.e., encoder and decoder share the same shape. T5 has 5 size options, small/base/large/3B/11B. However it is worth noting that T5 is trained upon a mixture of many tasks (more than BERT), we could roughly categorize them into denoising (any objective which refers to restoring a sentence from its broken version) and next-token-prediction. A discussion lasts for many years until today: could denoising present a implicit bias as good as NTP? A well-accepted conclusion is that denoising training objective works fine in a relative smaller scale (3B/11B?), it is less efficient in terms of training FLOPs, and which prevents it from scaling to larger scale. At present, the BERT series is largely deprecated, while T5 series are still active in some area, indicating that we don't want to deny the denoising training and bi-direction attention block thoroughly, however the causal LM has been proven to be more efficient nowadays.

#### GPT Series

GPT series are the most representative methods in language modeling (even more broadly, sequence to sequence modeling). Starting from GPT-1 released in 2018, to the most recent flagship models released in 2025, there has been a long history and some big progress in the development of GPT series. In my opinion, there are mainly three aspects to be noticed, the architecture, the training objectives, and the optimization carried out to enhance GPTs. The architecture is surprisingly simple, just causal decoder only LM, with the layers stacked many times to build a huge model, for example, GPT-3 has three versions, of sizes 1B, 6.7B, and 175B, released in July 2020, as a comparison, the Deepseek R1 has in total 675B parameters released in Jan 2025,  which is considered to be a non-trivial achievement just for training the model. GPT introduces the widely adopted pretrain-finetune training paradigm, in pretraining phase, we typically use an unsupervised NTP objective, while the supervised discriminative task is used for finetuning.   If we jump out of the traditional NLP perspective, and focus on the chat completion task, the pretraining problem reduces to selecting the corpus and adjusting the ratio of training batches among those corpus, and the finetuning problem reduces to making the model align with the human preference. The popular LLM distributions nowadays basically follow the GPT design style (if not the same), so we would not list them one by one.

## Large Language Models: Training

### Pre-training

We have briefly discussed the pre-training techniques of these models in the first section, now we would dive deeper into the details of these training paradigms, analyze the characteristics of each algorithm, and try to present a blueprint of the LLM pre-training world.



## Large Language Models: Inference

## Large Language Models: Optimization

